{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8fc4f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2ff487cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be6a3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gemini-2.0-flash'\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "GOOGLE_GEMINI_KEY = os.environ['GOOGLEAI_API_KEY']\n",
    "WEATHER_API_KEY = os.environ['WEATHER_API_KEY'] # https://www.weatherapi.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ce7ac3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather in a given location.\"\"\"\n",
    "    url = f'http://api.weatherapi.com/v1/current.json?key={WEATHER_API_KEY}&q={location}&aqi=no'\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        # Get current weather temperature in celsius\n",
    "        return f\"Current temperature for {location} is {data['current']['temp_c']}\"\n",
    "    else:\n",
    "        return f\"Não foi possível obter a previsão do tempo para {location}C.\"\n",
    "\n",
    "tools = [convert_to_openai_tool(get_current_weather)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6cdb4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(model=MODEL, google_api_key=GOOGLE_GEMINI_KEY)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "chain = prompt | llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f2dbed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/ai-study/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/opt/miniconda3/envs/ai-study/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/miniconda3/envs/ai-study/lib/python3.10/site-packages/gradio/blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/miniconda3/envs/ai-study/lib/python3.10/site-packages/gradio/blocks.py\", line 1660, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"/opt/miniconda3/envs/ai-study/lib/python3.10/site-packages/gradio/utils.py\", line 851, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/ai-study/lib/python3.10/site-packages/gradio/chat_interface.py\", line 876, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/miniconda3/envs/ai-study/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/opt/miniconda3/envs/ai-study/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/miniconda3/envs/ai-study/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/lb/48wqmhk57v97300d6q97s2tw0000gn/T/ipykernel_8469/1032266746.py\", line 4, in chat\n",
      "    for human, ai in history:\n",
      "ValueError: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "def chat(message, history):\n",
    "    # Converta o histórico do Gradio para o formato LangChain\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append(HumanMessage(content=human))\n",
    "        chat_history.append(AIMessage(content=ai))\n",
    "\n",
    "    # Adicione a nova mensagem\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "\n",
    "    # Invoque a cadeia\n",
    "    response = chain.invoke({\"input\": message, \"chat_history\": chat_history})\n",
    "\n",
    "    # Verifique se há chamadas de ferramentas\n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        for tool_call in response.tool_calls:\n",
    "            function_name = tool_call['name']\n",
    "            args = tool_call['args']\n",
    "            if function_name == 'get_current_weather':\n",
    "                weather_result = get_current_weather(**args)\n",
    "                return weather_result\n",
    "    else:\n",
    "        # Se não houver tool_calls, retorne o conteúdo da resposta\n",
    "        return response.content\n",
    "\n",
    "# # Teste a função\n",
    "# query = \"how is the weather in Bauru?\"\n",
    "# history = [\n",
    "#     (\"Hello!\", \"Hi! How can I assist you today?\"),\n",
    "# ]\n",
    "# result = chat(query, history)\n",
    "# print(\"\\nAnswer:\", result)\n",
    "app = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    type=\"messages\",\n",
    ").launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
